{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import min,max\n",
    "import re\n",
    "import csv\n",
    "\n",
    "# Creating spark session\n",
    "\n",
    "conf = SparkConf().setAppName(\"SparkCodingChallenge\").setMaster(\"local[*]\")\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "sqlC = SQLContext(sc)\n",
    "\n",
    "df = sqlC.read.csv(\"../data/iris.csv\")\n",
    "\n",
    "# rename column names\n",
    "\n",
    "newcolnames = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"class\"]\n",
    "\n",
    "for c,n in zip(df.columns,newcolnames):\n",
    "    df=df.withColumnRenamed(c,n)\n",
    "\n",
    "df = df.select(df.sepal_length.cast('double'),df.sepal_width.cast('double'),df.petal_length.cast('double'),df.petal_width.cast('double'),'class')\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "feature_cols = df.columns[:-1]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "df = assembler.transform(df)\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "df = df.select(['features','class'])\n",
    "label_indexer = StringIndexer(inputCol='class', outputCol='label').fit(df)\n",
    "df = label_indexer.transform(df)\n",
    "\n",
    "df2 = df.select(['features','label'])\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "lrModel = lr.fit(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.appName('Iris').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data = spark.createDataFrame(\n",
    "[(5.1,3.5,1.4,0.2),\n",
    "(6.2,3.4,5.4,2.3)],\n",
    "[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sepal_length: double (nullable = true)\n",
      " |-- sepal_width: double (nullable = true)\n",
      " |-- petal_length: double (nullable = true)\n",
      " |-- petal_width: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols2 = pred_data.columns\n",
    "assembler2 = VectorAssembler(inputCols=feature_cols2, outputCol='features')\n",
    "pred_df = assembler2.transform(pred_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-----------------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|         features|\n",
      "+------------+-----------+------------+-----------+-----------------+\n",
      "|         5.1|        3.5|         1.4|        0.2|[5.1,3.5,1.4,0.2]|\n",
      "|         6.2|        3.4|         5.4|        2.3|[6.2,3.4,5.4,2.3]|\n",
      "+------------+-----------+------------+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df2 = pred_df.drop('sepal_length','sepal_width','petal_length','petal_width')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|         features|\n",
      "+-----------------+\n",
      "|[5.1,3.5,1.4,0.2]|\n",
      "|[6.2,3.4,5.4,2.3]|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_df2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df2 = lrModel.transform(pred_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------------------+----------+\n",
      "|         features|       rawPrediction|         probability|prediction|\n",
      "+-----------------+--------------------+--------------------+----------+\n",
      "|[5.1,3.5,1.4,0.2]|[5.42426204068578...|[0.98060989835204...|       0.0|\n",
      "|[6.2,3.4,5.4,2.3]|[-4.7743046300876...|[2.49038896949521...|       2.0|\n",
      "+-----------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import IndexToString\n",
    "\n",
    "idx_to_string = IndexToString(inputCol=\"label\", outputCol=\"labelValue\")\n",
    "\n",
    "class_dict = idx_to_string.transform(df).drop(\"features\",\"class\").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+\n",
      "|label|     labelValue|\n",
      "+-----+---------------+\n",
      "|  0.0|    Iris-setosa|\n",
      "|  1.0|Iris-versicolor|\n",
      "|  2.0| Iris-virginica|\n",
      "+-----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_dict.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict = class_dict.selectExpr(\"label as prediction\",\"labelValue as predictionValue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+\n",
      "|prediction|predictionValue|\n",
      "+----------+---------------+\n",
      "|       0.0|    Iris-setosa|\n",
      "|       1.0|Iris-versicolor|\n",
      "|       2.0| Iris-virginica|\n",
      "+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_dict.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "pred_df2 = pred_df2.join(class_dict, pred_df2.prediction==class_dict.prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------------------+----------+----------+---------------+\n",
      "|         features|       rawPrediction|         probability|prediction|prediction|predictionValue|\n",
      "+-----------------+--------------------+--------------------+----------+----------+---------------+\n",
      "|[5.1,3.5,1.4,0.2]|[5.42426204068578...|[0.98060989835204...|       0.0|       0.0|    Iris-setosa|\n",
      "|[6.2,3.4,5.4,2.3]|[-4.7743046300876...|[2.49038896949521...|       2.0|       2.0| Iris-virginica|\n",
      "+-----------------+--------------------+--------------------+----------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionValues = pred_df2.select(\"predictionValue\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../out/out_3_2.txt', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"class\"])\n",
    "    for val in predictionValues:\n",
    "        writer.writerow([val.predictionValue])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
